{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Study\\\\RMIT\\\\2022\\\\COSC2793 - Computational Machine Learning\\\\Assignment02\\\\CML-Assign2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('D:\\\\Study\\\\RMIT\\\\2022\\\\COSC2793 - Computational Machine Learning\\\\Assignment02\\\\CML-Assign2')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "from final_simulation._env.real_intersection import RealIntersection8, RealIntersection9, RealIntersection10, RealIntersection11\n",
    "from final_simulation._sumo.simplest_intersection_simulation import SignalStates, SumoSimulation\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from final_simulation._env.callbacks import SaveOnBestTrainingRewardCallback\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward 08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"final_simulation\\\\_models\\\\reward_08\\\\ppo\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Sim time 1 minute\n",
    "max_simulation_seconds = 90\n",
    "number_episodes = 500\n",
    "\n",
    "# Save best check frequency\n",
    "check_freq = max_simulation_seconds * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simulation\n",
    "simulation = SumoSimulation(\n",
    "    sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo\",\n",
    "    #sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo-gui\",\n",
    "    sumo_config_path=\"C:\\\\sumoconfig\\\\real_intersection.sumocfg\",\n",
    "    signal_states=SignalStates\n",
    ")\n",
    "\n",
    "# Define environment\n",
    "env = RealIntersection8(\n",
    "    simulation=simulation,\n",
    "    max_simulation_seconds=max_simulation_seconds\n",
    ")\n",
    "\n",
    "# Logs will be saved in log_dir/monitor.csv\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "# Callback\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=check_freq,log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent\n",
    "model = PPO('MultiInputPolicy', env, verbose=1, device='auto', learning_rate=0.01)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(max_simulation_seconds*number_episodes),callback=callback)\n",
    "model.save(\"{0}\\\\final_model\".format(log_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward 09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"final_simulation\\\\_models\\\\reward_09\\\\ppo\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Sim time 1 minute\n",
    "max_simulation_seconds = 90\n",
    "number_episodes = 1000\n",
    "\n",
    "# Save best check frequency\n",
    "check_freq = max_simulation_seconds * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simulation\n",
    "simulation = SumoSimulation(\n",
    "    sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo\",\n",
    "    #sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo-gui\",\n",
    "    sumo_config_path=\"C:\\\\sumoconfig\\\\real_intersection.sumocfg\",\n",
    "    signal_states=SignalStates\n",
    ")\n",
    "\n",
    "# Define environment\n",
    "env = RealIntersection9(\n",
    "    simulation=simulation,\n",
    "    max_simulation_seconds=max_simulation_seconds\n",
    ")\n",
    "\n",
    "# Logs will be saved in log_dir/monitor.csv\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "# Callback\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=check_freq,log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent\n",
    "model = PPO('MultiInputPolicy', env, verbose=1, device='auto', learning_rate=0.001)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(max_simulation_seconds*number_episodes),callback=callback)\n",
    "model.save(\"{0}\\\\final_model\".format(log_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"final_simulation\\\\_models\\\\reward_10\\\\ppo\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Sim time 1 minute\n",
    "max_simulation_seconds = 90\n",
    "number_episodes = 1000\n",
    "\n",
    "# Save best check frequency\n",
    "check_freq = max_simulation_seconds * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simulation\n",
    "simulation = SumoSimulation(\n",
    "    sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo\",\n",
    "    #sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo-gui\",\n",
    "    sumo_config_path=\"C:\\\\sumoconfig\\\\real_intersection.sumocfg\",\n",
    "    signal_states=SignalStates\n",
    ")\n",
    "\n",
    "# Define environment\n",
    "env = RealIntersection10(\n",
    "    simulation=simulation,\n",
    "    max_simulation_seconds=max_simulation_seconds\n",
    ")\n",
    "\n",
    "# Logs will be saved in log_dir/monitor.csv\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "# Callback\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=check_freq,log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent\n",
    "model = PPO('MultiInputPolicy', env, verbose=1, device='auto', learning_rate=0.001)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(max_simulation_seconds*number_episodes),callback=callback)\n",
    "model.save(\"{0}\\\\final_model\".format(log_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"final_simulation\\\\_models\\\\reward_11\\\\ppo\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Sim time 1 minute\n",
    "max_simulation_seconds = 180\n",
    "number_episodes = 500\n",
    "\n",
    "# Save best check frequency\n",
    "check_freq = max_simulation_seconds * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simulation\n",
    "simulation = SumoSimulation(\n",
    "    sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo\",\n",
    "    #sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo-gui\",\n",
    "    sumo_config_path=\"C:\\\\sumoconfig\\\\real_intersection.sumocfg\",\n",
    "    signal_states=SignalStates\n",
    ")\n",
    "\n",
    "# Define environment\n",
    "env = RealIntersection11(\n",
    "    simulation=simulation,\n",
    "    max_simulation_seconds=max_simulation_seconds\n",
    ")\n",
    "\n",
    "# Logs will be saved in log_dir/monitor.csv\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "# Callback\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=check_freq,log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 1800\n",
      "Best mean reward: -inf - Last mean reward per episode: -44360.90\n",
      "Saving new best model to final_simulation\\_models\\reward_11\\ppo\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 179       |\n",
      "|    ep_rew_mean     | -4.26e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 5         |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 387       |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 3600\n",
      "Best mean reward: -44360.90 - Last mean reward per episode: -46644.15\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 179          |\n",
      "|    ep_rew_mean          | -4.61e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 813          |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029697823 |\n",
      "|    clip_fraction        | 0.00249      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.08        |\n",
      "|    explained_variance   | -0.000126    |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 1.09e+07     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00374     |\n",
      "|    value_loss           | 2.25e+07     |\n",
      "------------------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 5400\n",
      "Best mean reward: -44360.90 - Last mean reward per episode: -45931.37\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 179         |\n",
      "|    ep_rew_mean          | -4.46e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 5           |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 1179        |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003913885 |\n",
      "|    clip_fraction        | 0.00981     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.07       |\n",
      "|    explained_variance   | 1.59e-05    |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 1.45e+07    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00488    |\n",
      "|    value_loss           | 3.25e+07    |\n",
      "-----------------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 7200\n",
      "Best mean reward: -44360.90 - Last mean reward per episode: -41402.95\n",
      "Saving new best model to final_simulation\\_models\\reward_11\\ppo\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 179          |\n",
      "|    ep_rew_mean          | -3.97e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 1537         |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076419534 |\n",
      "|    clip_fraction        | 0.0581       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.05        |\n",
      "|    explained_variance   | 3.1e-06      |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 9.73e+06     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00903     |\n",
      "|    value_loss           | 1.92e+07     |\n",
      "------------------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 9000\n",
      "Best mean reward: -41402.95 - Last mean reward per episode: -38348.62\n",
      "Saving new best model to final_simulation\\_models\\reward_11\\ppo\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 179         |\n",
      "|    ep_rew_mean          | -3.68e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 5           |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 1888        |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006657953 |\n",
      "|    clip_fraction        | 0.0673      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.03       |\n",
      "|    explained_variance   | -5.13e-06   |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 3.25e+06    |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    value_loss           | 6.74e+06    |\n",
      "-----------------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 10800\n",
      "Best mean reward: -38348.62 - Last mean reward per episode: -35860.88\n",
      "Saving new best model to final_simulation\\_models\\reward_11\\ppo\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 179          |\n",
      "|    ep_rew_mean          | -3.37e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 2215         |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032443164 |\n",
      "|    clip_fraction        | 0.00225      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.02        |\n",
      "|    explained_variance   | 5.36e-07     |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 4.13e+06     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00785     |\n",
      "|    value_loss           | 8.56e+06     |\n",
      "------------------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 12600\n",
      "Best mean reward: -35860.88 - Last mean reward per episode: -33215.53\n",
      "Saving new best model to final_simulation\\_models\\reward_11\\ppo\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 179          |\n",
      "|    ep_rew_mean          | -3.08e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 2542         |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0083231535 |\n",
      "|    clip_fraction        | 0.0332       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.01        |\n",
      "|    explained_variance   | 1.19e-06     |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 1.27e+06     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0125      |\n",
      "|    value_loss           | 2.74e+06     |\n",
      "------------------------------------------\n",
      "Num timesteps: 14400\n",
      "Best mean reward: -33215.53 - Last mean reward per episode: -30825.50\n",
      "Saving new best model to final_simulation\\_models\\reward_11\\ppo\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 16200\n",
      "Best mean reward: -30825.50 - Last mean reward per episode: -28870.06\n",
      "Saving new best model to final_simulation\\_models\\reward_11\\ppo\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 179         |\n",
      "|    ep_rew_mean          | -2.87e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 5           |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 2859        |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011425311 |\n",
      "|    clip_fraction        | 0.0954      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.95       |\n",
      "|    explained_variance   | 8.52e-06    |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 8.56e+05    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    value_loss           | 1.96e+06    |\n",
      "-----------------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 18000\n",
      "Best mean reward: -28870.06 - Last mean reward per episode: -27224.15\n",
      "Saving new best model to final_simulation\\_models\\reward_11\\ppo\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 179          |\n",
      "|    ep_rew_mean          | -2.66e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 3171         |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0095445225 |\n",
      "|    clip_fraction        | 0.052        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.91        |\n",
      "|    explained_variance   | 4.77e-07     |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 8.04e+05     |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.0153      |\n",
      "|    value_loss           | 1.52e+06     |\n",
      "------------------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 19800\n",
      "Best mean reward: -27224.15 - Last mean reward per episode: -23844.31\n",
      "Saving new best model to final_simulation\\_models\\reward_11\\ppo\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 179         |\n",
      "|    ep_rew_mean          | -2.24e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 5           |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 3470        |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008750581 |\n",
      "|    clip_fraction        | 0.059       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.84       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 5.89e+05    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    value_loss           | 1.22e+06    |\n",
      "-----------------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 21600\n",
      "Best mean reward: -23844.31 - Last mean reward per episode: -20007.85\n",
      "Saving new best model to final_simulation\\_models\\reward_11\\ppo\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 179         |\n",
      "|    ep_rew_mean          | -1.81e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 5           |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 3769        |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008595981 |\n",
      "|    clip_fraction        | 0.0443      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.83       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 4.07e+05    |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 8.55e+05    |\n",
      "-----------------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 23400\n",
      "Best mean reward: -20007.85 - Last mean reward per episode: -16444.42\n",
      "Saving new best model to final_simulation\\_models\\reward_11\\ppo\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 179         |\n",
      "|    ep_rew_mean          | -1.49e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 6           |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 4066        |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007078663 |\n",
      "|    clip_fraction        | 0.0268      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.76       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 3.44e+05    |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    value_loss           | 7.35e+05    |\n",
      "-----------------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 25200\n",
      "Best mean reward: -16444.42 - Last mean reward per episode: -14572.08\n",
      "Saving new best model to final_simulation\\_models\\reward_11\\ppo\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 179          |\n",
      "|    ep_rew_mean          | -1.3e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 6            |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 4362         |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074187955 |\n",
      "|    clip_fraction        | 0.0318       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.73        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 3.27e+05     |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.0136      |\n",
      "|    value_loss           | 6.19e+05     |\n",
      "------------------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 27000\n",
      "Best mean reward: -14572.08 - Last mean reward per episode: -12761.74\n",
      "Saving new best model to final_simulation\\_models\\reward_11\\ppo\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 179        |\n",
      "|    ep_rew_mean          | -1.13e+04  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 6          |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 4656       |\n",
      "|    total_timesteps      | 28672      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00959279 |\n",
      "|    clip_fraction        | 0.0783     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.62      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 2.14e+05   |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.0169    |\n",
      "|    value_loss           | 4.68e+05   |\n",
      "----------------------------------------\n",
      "Num timesteps: 28800\n",
      "Best mean reward: -12761.74 - Last mean reward per episode: -11323.12\n",
      "Saving new best model to final_simulation\\_models\\reward_11\\ppo\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 30600\n",
      "Best mean reward: -11323.12 - Last mean reward per episode: -10273.40\n",
      "Saving new best model to final_simulation\\_models\\reward_11\\ppo\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 179         |\n",
      "|    ep_rew_mean          | -1.02e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 6           |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 4950        |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012178963 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 2.63e+05    |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    value_loss           | 6.32e+05    |\n",
      "-----------------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 32400\n",
      "Best mean reward: -10273.40 - Last mean reward per episode: -9511.59\n",
      "Saving new best model to final_simulation\\_models\\reward_11\\ppo\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 179         |\n",
      "|    ep_rew_mean          | -9.37e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 6           |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 5235        |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013551168 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 1.5e+05     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    value_loss           | 3.26e+05    |\n",
      "-----------------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 34200\n",
      "Best mean reward: -9511.59 - Last mean reward per episode: -8815.47\n",
      "Saving new best model to final_simulation\\_models\\reward_11\\ppo\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 179         |\n",
      "|    ep_rew_mean          | -8.57e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 6           |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 5520        |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010328818 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 1.77e+05    |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    value_loss           | 3.01e+05    |\n",
      "-----------------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 36000\n",
      "Best mean reward: -8815.47 - Last mean reward per episode: -8073.08\n",
      "Saving new best model to final_simulation\\_models\\reward_11\\ppo\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 179         |\n",
      "|    ep_rew_mean          | -7.89e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 6           |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 5805        |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009671565 |\n",
      "|    clip_fraction        | 0.0832      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 1.16e+05    |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    value_loss           | 2.43e+05    |\n",
      "-----------------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 37800\n",
      "Best mean reward: -8073.08 - Last mean reward per episode: -7587.27\n",
      "Saving new best model to final_simulation\\_models\\reward_11\\ppo\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 179          |\n",
      "|    ep_rew_mean          | -7.18e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 6            |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 6087         |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0086547425 |\n",
      "|    clip_fraction        | 0.1          |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.16        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 9.87e+04     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0225      |\n",
      "|    value_loss           | 1.81e+05     |\n",
      "------------------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 39600\n",
      "Best mean reward: -7587.27 - Last mean reward per episode: -7033.59\n",
      "Saving new best model to final_simulation\\_models\\reward_11\\ppo\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 179         |\n",
      "|    ep_rew_mean          | -6.7e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 6           |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 6366        |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010458702 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 9.04e+04    |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0219     |\n",
      "|    value_loss           | 1.69e+05    |\n",
      "-----------------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 41400\n",
      "Best mean reward: -7033.59 - Last mean reward per episode: -6553.47\n",
      "Saving new best model to final_simulation\\_models\\reward_11\\ppo\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 179         |\n",
      "|    ep_rew_mean          | -6.14e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 6           |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 6648        |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011103525 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 5.07e+04    |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    value_loss           | 1.25e+05    |\n",
      "-----------------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 43200\n",
      "Best mean reward: -6553.47 - Last mean reward per episode: -6110.75\n",
      "Saving new best model to final_simulation\\_models\\reward_11\\ppo\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n"
     ]
    }
   ],
   "source": [
    "# Define agent\n",
    "model = PPO('MultiInputPolicy', env, verbose=1, device='auto', learning_rate=0.001)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(max_simulation_seconds*number_episodes),callback=callback)\n",
    "model.save(\"{0}\\\\final_model\".format(log_dir))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f344f29490a86472adfcb0d4c68c5dfdcf97bca003fdba9df268b07cb4a6c3e0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 ('_penv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
