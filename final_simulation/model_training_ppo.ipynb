{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('D:\\\\Study\\\\RMIT\\\\2022\\\\COSC2793 - Computational Machine Learning\\\\Assignment02\\\\CML-Assign2')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from final_simulation._env.real_intersection import RealIntersection8, RealIntersection9, RealIntersection10, RealIntersection11, RealIntersectionSimpleObs12\n",
    "from final_simulation._sumo.simplest_intersection_simulation import SignalStates, SumoSimulation, SumoSimulationSimpleObs\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from final_simulation._env.callbacks import SaveOnBestTrainingRewardCallback\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward 08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"final_simulation\\\\_models\\\\reward_08\\\\ppo\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Sim time 1 minute\n",
    "max_simulation_seconds = 90\n",
    "number_episodes = 500\n",
    "\n",
    "# Save best check frequency\n",
    "check_freq = max_simulation_seconds * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simulation\n",
    "simulation = SumoSimulation(\n",
    "    sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo\",\n",
    "    #sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo-gui\",\n",
    "    sumo_config_path=\"C:\\\\sumoconfig\\\\real_intersection.sumocfg\",\n",
    "    signal_states=SignalStates\n",
    ")\n",
    "\n",
    "# Define environment\n",
    "env = RealIntersection8(\n",
    "    simulation=simulation,\n",
    "    max_simulation_seconds=max_simulation_seconds\n",
    ")\n",
    "\n",
    "# Logs will be saved in log_dir/monitor.csv\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "# Callback\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=check_freq,log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent\n",
    "model = PPO('MultiInputPolicy', env, verbose=1, device='auto', learning_rate=0.01)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(max_simulation_seconds*number_episodes),callback=callback)\n",
    "model.save(\"{0}\\\\final_model\".format(log_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward 09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"final_simulation\\\\_models\\\\reward_09\\\\ppo\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Sim time 1 minute\n",
    "max_simulation_seconds = 90\n",
    "number_episodes = 1000\n",
    "\n",
    "# Save best check frequency\n",
    "check_freq = max_simulation_seconds * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simulation\n",
    "simulation = SumoSimulation(\n",
    "    sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo\",\n",
    "    #sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo-gui\",\n",
    "    sumo_config_path=\"C:\\\\sumoconfig\\\\real_intersection.sumocfg\",\n",
    "    signal_states=SignalStates\n",
    ")\n",
    "\n",
    "# Define environment\n",
    "env = RealIntersection9(\n",
    "    simulation=simulation,\n",
    "    max_simulation_seconds=max_simulation_seconds\n",
    ")\n",
    "\n",
    "# Logs will be saved in log_dir/monitor.csv\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "# Callback\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=check_freq,log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent\n",
    "model = PPO('MultiInputPolicy', env, verbose=1, device='auto', learning_rate=0.001)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(max_simulation_seconds*number_episodes),callback=callback)\n",
    "model.save(\"{0}\\\\final_model\".format(log_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"final_simulation\\\\_models\\\\reward_10\\\\ppo\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Sim time 1 minute\n",
    "max_simulation_seconds = 90\n",
    "number_episodes = 1000\n",
    "\n",
    "# Save best check frequency\n",
    "check_freq = max_simulation_seconds * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simulation\n",
    "simulation = SumoSimulation(\n",
    "    sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo\",\n",
    "    #sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo-gui\",\n",
    "    sumo_config_path=\"C:\\\\sumoconfig\\\\real_intersection.sumocfg\",\n",
    "    signal_states=SignalStates\n",
    ")\n",
    "\n",
    "# Define environment\n",
    "env = RealIntersection10(\n",
    "    simulation=simulation,\n",
    "    max_simulation_seconds=max_simulation_seconds\n",
    ")\n",
    "\n",
    "# Logs will be saved in log_dir/monitor.csv\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "# Callback\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=check_freq,log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent\n",
    "model = PPO('MultiInputPolicy', env, verbose=1, device='auto', learning_rate=0.001)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(max_simulation_seconds*number_episodes),callback=callback)\n",
    "model.save(\"{0}\\\\final_model\".format(log_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"final_simulation\\\\_models\\\\reward_11\\\\ppo\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Sim time 1 minute\n",
    "max_simulation_seconds = 180\n",
    "number_episodes = 500\n",
    "\n",
    "# Save best check frequency\n",
    "check_freq = max_simulation_seconds * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simulation\n",
    "simulation = SumoSimulation(\n",
    "    sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo\",\n",
    "    #sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo-gui\",\n",
    "    sumo_config_path=\"C:\\\\sumoconfig\\\\real_intersection.sumocfg\",\n",
    "    signal_states=SignalStates\n",
    ")\n",
    "\n",
    "# Define environment\n",
    "env = RealIntersection11(\n",
    "    simulation=simulation,\n",
    "    max_simulation_seconds=max_simulation_seconds\n",
    ")\n",
    "\n",
    "# Logs will be saved in log_dir/monitor.csv\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "# Callback\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=check_freq,log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent\n",
    "model = PPO('MultiInputPolicy', env, verbose=1, device='auto', learning_rate=0.001)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(max_simulation_seconds*number_episodes),callback=callback)\n",
    "model.save(\"{0}\\\\final_model\".format(log_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward 12 (Simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"final_simulation\\\\_models\\\\reward_12\\\\simple\\\\ppo\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Sim time 1 minute\n",
    "max_simulation_seconds = 180\n",
    "number_episodes = 500\n",
    "\n",
    "# Save best check frequency\n",
    "check_freq = max_simulation_seconds * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simulation\n",
    "simulation = SumoSimulationSimpleObs(\n",
    "    sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo\",\n",
    "    #sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo-gui\",\n",
    "    sumo_config_path=\"C:\\\\sumoconfig\\\\real_intersection.sumocfg\",\n",
    "    signal_states=SignalStates\n",
    ")\n",
    "\n",
    "# Define environment\n",
    "env = RealIntersectionSimpleObs12(\n",
    "    simulation=simulation,\n",
    "    max_simulation_seconds=max_simulation_seconds\n",
    ")\n",
    "\n",
    "# Logs will be saved in log_dir/monitor.csv\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "# Callback\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=check_freq,log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "No simulation running.\n"
     ]
    }
   ],
   "source": [
    "# Define agent\n",
    "model = PPO('MlpPolicy', env, verbose=1, device='auto', learning_rate=0.001)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(max_simulation_seconds*number_episodes),callback=callback)\n",
    "model.save(\"{0}\\\\final_model\".format(log_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f344f29490a86472adfcb0d4c68c5dfdcf97bca003fdba9df268b07cb4a6c3e0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 ('_penv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
