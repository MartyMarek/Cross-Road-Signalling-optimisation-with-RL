{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Study\\\\RMIT\\\\2022\\\\COSC2793 - Computational Machine Learning\\\\Assignment02\\\\CML-Assign2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('D:\\\\Study\\\\RMIT\\\\2022\\\\COSC2793 - Computational Machine Learning\\\\Assignment02\\\\CML-Assign2')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from final_simulation._env.real_intersection import RealIntersection8, RealIntersection9, RealIntersection10, RealIntersection11, RealIntersectionSimpleObs12, RealIntersectionSimpleObs13\n",
    "from final_simulation._sumo.simplest_intersection_simulation import SignalStates, SumoSimulation, SumoSimulationSimpleObs\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from final_simulation._env.callbacks import SaveOnBestTrainingRewardCallback\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward 08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"final_simulation\\\\_models\\\\reward_08\\\\ppo\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Sim time 1 minute\n",
    "max_simulation_seconds = 90\n",
    "number_episodes = 500\n",
    "\n",
    "# Save best check frequency\n",
    "check_freq = max_simulation_seconds * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simulation\n",
    "simulation = SumoSimulation(\n",
    "    sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo\",\n",
    "    #sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo-gui\",\n",
    "    sumo_config_path=\"C:\\\\sumoconfig\\\\real_intersection.sumocfg\",\n",
    "    signal_states=SignalStates\n",
    ")\n",
    "\n",
    "# Define environment\n",
    "env = RealIntersection8(\n",
    "    simulation=simulation,\n",
    "    max_simulation_seconds=max_simulation_seconds\n",
    ")\n",
    "\n",
    "# Logs will be saved in log_dir/monitor.csv\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "# Callback\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=check_freq,log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent\n",
    "model = PPO('MultiInputPolicy', env, verbose=1, device='auto', learning_rate=0.01)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(max_simulation_seconds*number_episodes),callback=callback)\n",
    "model.save(\"{0}\\\\final_model\".format(log_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward 09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"final_simulation\\\\_models\\\\reward_09\\\\ppo\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Sim time 1 minute\n",
    "max_simulation_seconds = 90\n",
    "number_episodes = 1000\n",
    "\n",
    "# Save best check frequency\n",
    "check_freq = max_simulation_seconds * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simulation\n",
    "simulation = SumoSimulation(\n",
    "    sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo\",\n",
    "    #sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo-gui\",\n",
    "    sumo_config_path=\"C:\\\\sumoconfig\\\\real_intersection.sumocfg\",\n",
    "    signal_states=SignalStates\n",
    ")\n",
    "\n",
    "# Define environment\n",
    "env = RealIntersection9(\n",
    "    simulation=simulation,\n",
    "    max_simulation_seconds=max_simulation_seconds\n",
    ")\n",
    "\n",
    "# Logs will be saved in log_dir/monitor.csv\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "# Callback\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=check_freq,log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent\n",
    "model = PPO('MultiInputPolicy', env, verbose=1, device='auto', learning_rate=0.001)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(max_simulation_seconds*number_episodes),callback=callback)\n",
    "model.save(\"{0}\\\\final_model\".format(log_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"final_simulation\\\\_models\\\\reward_10\\\\ppo\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Sim time 1 minute\n",
    "max_simulation_seconds = 90\n",
    "number_episodes = 1000\n",
    "\n",
    "# Save best check frequency\n",
    "check_freq = max_simulation_seconds * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simulation\n",
    "simulation = SumoSimulation(\n",
    "    sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo\",\n",
    "    #sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo-gui\",\n",
    "    sumo_config_path=\"C:\\\\sumoconfig\\\\real_intersection.sumocfg\",\n",
    "    signal_states=SignalStates\n",
    ")\n",
    "\n",
    "# Define environment\n",
    "env = RealIntersection10(\n",
    "    simulation=simulation,\n",
    "    max_simulation_seconds=max_simulation_seconds\n",
    ")\n",
    "\n",
    "# Logs will be saved in log_dir/monitor.csv\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "# Callback\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=check_freq,log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent\n",
    "model = PPO('MultiInputPolicy', env, verbose=1, device='auto', learning_rate=0.001)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(max_simulation_seconds*number_episodes),callback=callback)\n",
    "model.save(\"{0}\\\\final_model\".format(log_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"final_simulation\\\\_models\\\\reward_11\\\\ppo\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Sim time 1 minute\n",
    "max_simulation_seconds = 180\n",
    "number_episodes = 500\n",
    "\n",
    "# Save best check frequency\n",
    "check_freq = max_simulation_seconds * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simulation\n",
    "simulation = SumoSimulation(\n",
    "    sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo\",\n",
    "    #sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo-gui\",\n",
    "    sumo_config_path=\"C:\\\\sumoconfig\\\\real_intersection.sumocfg\",\n",
    "    signal_states=SignalStates\n",
    ")\n",
    "\n",
    "# Define environment\n",
    "env = RealIntersection11(\n",
    "    simulation=simulation,\n",
    "    max_simulation_seconds=max_simulation_seconds\n",
    ")\n",
    "\n",
    "# Logs will be saved in log_dir/monitor.csv\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "# Callback\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=check_freq,log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent\n",
    "model = PPO('MultiInputPolicy', env, verbose=1, device='auto', learning_rate=0.001)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(max_simulation_seconds*number_episodes),callback=callback)\n",
    "model.save(\"{0}\\\\final_model\".format(log_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward 12 (Simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"final_simulation\\\\_models\\\\reward_12\\\\simple\\\\ppo\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Sim time 1 minute\n",
    "max_simulation_seconds = 180\n",
    "number_episodes = 500\n",
    "\n",
    "# Save best check frequency\n",
    "check_freq = max_simulation_seconds * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simulation\n",
    "simulation = SumoSimulationSimpleObs(\n",
    "    sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo\",\n",
    "    #sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo-gui\",\n",
    "    sumo_config_path=\"C:\\\\sumoconfig\\\\real_intersection.sumocfg\",\n",
    "    signal_states=SignalStates\n",
    ")\n",
    "\n",
    "# Define environment\n",
    "env = RealIntersectionSimpleObs12(\n",
    "    simulation=simulation,\n",
    "    max_simulation_seconds=max_simulation_seconds\n",
    ")\n",
    "\n",
    "# Logs will be saved in log_dir/monitor.csv\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "# Callback\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=check_freq,log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent\n",
    "model = PPO('MlpPolicy', env, verbose=1, device='auto', learning_rate=0.001)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(max_simulation_seconds*number_episodes),callback=callback)\n",
    "model.save(\"{0}\\\\final_model\".format(log_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward 13 (Simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"final_simulation\\\\_models\\\\reward_13\\\\simple\\\\ppo\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Sim time 1 minute\n",
    "max_simulation_seconds = 180\n",
    "number_episodes = 1000\n",
    "\n",
    "# Save best check frequency\n",
    "check_freq = max_simulation_seconds * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simulation\n",
    "simulation = SumoSimulationSimpleObs(\n",
    "    sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo\",\n",
    "    #sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo-gui\",\n",
    "    sumo_config_path=\"C:\\\\sumoconfig\\\\real_intersection.sumocfg\",\n",
    "    signal_states=SignalStates\n",
    ")\n",
    "\n",
    "# Define environment\n",
    "env = RealIntersectionSimpleObs13(\n",
    "    simulation=simulation,\n",
    "    max_simulation_seconds=max_simulation_seconds\n",
    ")\n",
    "\n",
    "# Logs will be saved in log_dir/monitor.csv\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "# Callback\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=check_freq,log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent\n",
    "model = PPO('MlpPolicy', env, verbose=1, device='auto', learning_rate=0.001)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(max_simulation_seconds*number_episodes),callback=callback)\n",
    "model.save(\"{0}\\\\final_model\".format(log_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Training\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"final_simulation\\\\_models\\\\final\\\\ppo\\\\lr_0_1\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Sim time 1 minute\n",
    "max_simulation_seconds = 120\n",
    "number_episodes = 2000\n",
    "\n",
    "# Save best check frequency\n",
    "check_freq = max_simulation_seconds * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simulation\n",
    "simulation = SumoSimulationSimpleObs(\n",
    "    sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo\",\n",
    "    #sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo-gui\",\n",
    "    sumo_config_path=\"C:\\\\sumoconfig\\\\real_intersection.sumocfg\",\n",
    "    signal_states=SignalStates\n",
    ")\n",
    "\n",
    "# Define environment\n",
    "env = RealIntersectionSimpleObs13(\n",
    "    simulation=simulation,\n",
    "    max_simulation_seconds=max_simulation_seconds\n",
    ")\n",
    "\n",
    "# Logs will be saved in log_dir/monitor.csv\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "# Callback\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=check_freq,log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 1200\n",
      "Best mean reward: -inf - Last mean reward per episode: -3569.50\n",
      "Saving new best model to final_simulation\\_models\\final\\ppo\\lr_0_1\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 119       |\n",
      "|    ep_rew_mean     | -3.61e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 5         |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 345       |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 2400\n",
      "Best mean reward: -3569.50 - Last mean reward per episode: -3611.60\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 3600\n",
      "Best mean reward: -3569.50 - Last mean reward per episode: -3572.40\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 119       |\n",
      "|    ep_rew_mean          | -3.56e+03 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 5         |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 777       |\n",
      "|    total_timesteps      | 4096      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 28.931301 |\n",
      "|    clip_fraction        | 0.992     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0957   |\n",
      "|    explained_variance   | -0.00173  |\n",
      "|    learning_rate        | 0.1       |\n",
      "|    loss                 | 1.01e+03  |\n",
      "|    n_updates            | 10        |\n",
      "|    policy_gradient_loss | 0.604     |\n",
      "|    value_loss           | 3.58e+03  |\n",
      "---------------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 4800\n",
      "Best mean reward: -3569.50 - Last mean reward per episode: -3552.80\n",
      "Saving new best model to final_simulation\\_models\\final\\ppo\\lr_0_1\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 6000\n",
      "Best mean reward: -3552.80 - Last mean reward per episode: -3541.04\n",
      "Saving new best model to final_simulation\\_models\\final\\ppo\\lr_0_1\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 119       |\n",
      "|    ep_rew_mean          | -3.54e+03 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 5         |\n",
      "|    iterations           | 3         |\n",
      "|    time_elapsed         | 1210      |\n",
      "|    total_timesteps      | 6144      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.99e-08 |\n",
      "|    explained_variance   | -0.00101  |\n",
      "|    learning_rate        | 0.1       |\n",
      "|    loss                 | 7.79e+03  |\n",
      "|    n_updates            | 20        |\n",
      "|    policy_gradient_loss | -3.37e-09 |\n",
      "|    value_loss           | 1.35e+04  |\n",
      "---------------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 7200\n",
      "Best mean reward: -3541.04 - Last mean reward per episode: -3533.20\n",
      "Saving new best model to final_simulation\\_models\\final\\ppo\\lr_0_1\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 119       |\n",
      "|    ep_rew_mean          | -3.53e+03 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 4         |\n",
      "|    iterations           | 4         |\n",
      "|    time_elapsed         | 1647      |\n",
      "|    total_timesteps      | 8192      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.95e-08 |\n",
      "|    explained_variance   | 5.96e-08  |\n",
      "|    learning_rate        | 0.1       |\n",
      "|    loss                 | 5.93e+03  |\n",
      "|    n_updates            | 30        |\n",
      "|    policy_gradient_loss | 5.25e-10  |\n",
      "|    value_loss           | 1.35e+04  |\n",
      "---------------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 8400\n",
      "Best mean reward: -3533.20 - Last mean reward per episode: -3527.60\n",
      "Saving new best model to final_simulation\\_models\\final\\ppo\\lr_0_1\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 9600\n",
      "Best mean reward: -3527.60 - Last mean reward per episode: -3523.40\n",
      "Saving new best model to final_simulation\\_models\\final\\ppo\\lr_0_1\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 119       |\n",
      "|    ep_rew_mean          | -3.52e+03 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 4         |\n",
      "|    iterations           | 5         |\n",
      "|    time_elapsed         | 2082      |\n",
      "|    total_timesteps      | 10240     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.95e-08 |\n",
      "|    explained_variance   | 0.0226    |\n",
      "|    learning_rate        | 0.1       |\n",
      "|    loss                 | 7.59e+03  |\n",
      "|    n_updates            | 40        |\n",
      "|    policy_gradient_loss | -7.01e-10 |\n",
      "|    value_loss           | 1.31e+04  |\n",
      "---------------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 10800\n",
      "Best mean reward: -3523.40 - Last mean reward per episode: -3520.13\n",
      "Saving new best model to final_simulation\\_models\\final\\ppo\\lr_0_1\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 12000\n",
      "Best mean reward: -3520.13 - Last mean reward per episode: -3517.52\n",
      "Saving new best model to final_simulation\\_models\\final\\ppo\\lr_0_1\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 119       |\n",
      "|    ep_rew_mean          | -3.51e+03 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 4         |\n",
      "|    iterations           | 6         |\n",
      "|    time_elapsed         | 2508      |\n",
      "|    total_timesteps      | 12288     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.95e-08 |\n",
      "|    explained_variance   | 0.0241    |\n",
      "|    learning_rate        | 0.1       |\n",
      "|    loss                 | 7.94e+03  |\n",
      "|    n_updates            | 50        |\n",
      "|    policy_gradient_loss | -2.04e-10 |\n",
      "|    value_loss           | 1.31e+04  |\n",
      "---------------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 13200\n",
      "Best mean reward: -3517.52 - Last mean reward per episode: -3509.97\n",
      "Saving new best model to final_simulation\\_models\\final\\ppo\\lr_0_1\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 119       |\n",
      "|    ep_rew_mean          | -3.49e+03 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 4         |\n",
      "|    iterations           | 7         |\n",
      "|    time_elapsed         | 2937      |\n",
      "|    total_timesteps      | 14336     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.95e-08 |\n",
      "|    explained_variance   | 0.0232    |\n",
      "|    learning_rate        | 0.1       |\n",
      "|    loss                 | 8.86e+03  |\n",
      "|    n_updates            | 60        |\n",
      "|    policy_gradient_loss | -1.43e-09 |\n",
      "|    value_loss           | 1.32e+04  |\n",
      "---------------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 14400\n",
      "Best mean reward: -3509.97 - Last mean reward per episode: -3494.00\n",
      "Saving new best model to final_simulation\\_models\\final\\ppo\\lr_0_1\\best_model\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 15600\n",
      "Best mean reward: -3494.00 - Last mean reward per episode: -3494.00\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 119       |\n",
      "|    ep_rew_mean          | -3.49e+03 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 4         |\n",
      "|    iterations           | 8         |\n",
      "|    time_elapsed         | 3367      |\n",
      "|    total_timesteps      | 16384     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.95e-08 |\n",
      "|    explained_variance   | 0.0238    |\n",
      "|    learning_rate        | 0.1       |\n",
      "|    loss                 | 6.57e+03  |\n",
      "|    n_updates            | 70        |\n",
      "|    policy_gradient_loss | -5.94e-10 |\n",
      "|    value_loss           | 1.3e+04   |\n",
      "---------------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 16800\n",
      "Best mean reward: -3494.00 - Last mean reward per episode: -3494.00\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 18000\n",
      "Best mean reward: -3494.00 - Last mean reward per episode: -3494.00\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 119       |\n",
      "|    ep_rew_mean          | -3.49e+03 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 4         |\n",
      "|    iterations           | 9         |\n",
      "|    time_elapsed         | 3798      |\n",
      "|    total_timesteps      | 18432     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.95e-08 |\n",
      "|    explained_variance   | 0.0241    |\n",
      "|    learning_rate        | 0.1       |\n",
      "|    loss                 | 6.63e+03  |\n",
      "|    n_updates            | 80        |\n",
      "|    policy_gradient_loss | -3.49e-11 |\n",
      "|    value_loss           | 1.29e+04  |\n",
      "---------------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 19200\n",
      "Best mean reward: -3494.00 - Last mean reward per episode: -3494.00\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 20400\n",
      "Best mean reward: -3494.00 - Last mean reward per episode: -3494.00\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 119       |\n",
      "|    ep_rew_mean          | -3.49e+03 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 4         |\n",
      "|    iterations           | 10        |\n",
      "|    time_elapsed         | 4229      |\n",
      "|    total_timesteps      | 20480     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.95e-08 |\n",
      "|    explained_variance   | 0.0241    |\n",
      "|    learning_rate        | 0.1       |\n",
      "|    loss                 | 6.74e+03  |\n",
      "|    n_updates            | 90        |\n",
      "|    policy_gradient_loss | -3.28e-10 |\n",
      "|    value_loss           | 1.31e+04  |\n",
      "---------------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 21600\n",
      "Best mean reward: -3494.00 - Last mean reward per episode: -3494.00\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 119       |\n",
      "|    ep_rew_mean          | -3.49e+03 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 4         |\n",
      "|    iterations           | 11        |\n",
      "|    time_elapsed         | 4659      |\n",
      "|    total_timesteps      | 22528     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.95e-08 |\n",
      "|    explained_variance   | 0.0245    |\n",
      "|    learning_rate        | 0.1       |\n",
      "|    loss                 | 6.69e+03  |\n",
      "|    n_updates            | 100       |\n",
      "|    policy_gradient_loss | 8.61e-10  |\n",
      "|    value_loss           | 1.31e+04  |\n",
      "---------------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 22800\n",
      "Best mean reward: -3494.00 - Last mean reward per episode: -3494.00\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 24000\n",
      "Best mean reward: -3494.00 - Last mean reward per episode: -3494.00\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 119       |\n",
      "|    ep_rew_mean          | -3.49e+03 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 4         |\n",
      "|    iterations           | 12        |\n",
      "|    time_elapsed         | 5089      |\n",
      "|    total_timesteps      | 24576     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.95e-08 |\n",
      "|    explained_variance   | 0.0234    |\n",
      "|    learning_rate        | 0.1       |\n",
      "|    loss                 | 6.64e+03  |\n",
      "|    n_updates            | 110       |\n",
      "|    policy_gradient_loss | -4.79e-10 |\n",
      "|    value_loss           | 1.31e+04  |\n",
      "---------------------------------------\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "Num timesteps: 25200\n",
      "Best mean reward: -3494.00 - Last mean reward per episode: -3494.00\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "No simulation running.\n"
     ]
    }
   ],
   "source": [
    "# Define agent\n",
    "model = PPO('MlpPolicy', env, verbose=1, device='auto', learning_rate=0.1, gamma=0.9)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(max_simulation_seconds*number_episodes),callback=callback)\n",
    "model.save(\"{0}\\\\final_model\".format(log_dir))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f344f29490a86472adfcb0d4c68c5dfdcf97bca003fdba9df268b07cb4a6c3e0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 ('_penv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
