{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Study\\\\RMIT\\\\2022\\\\COSC2793 - Computational Machine Learning\\\\Assignment02\\\\CML-Assign2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "cfd = Path(os.getcwd())\n",
    "os.chdir(cfd.parent.absolute())\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from final_simulation._env.real_intersection import RealIntersectionSimpleObs13, RealIntersectionSimpleObs13_Static\n",
    "from final_simulation._sumo.simplest_intersection_simulation import SignalStates, SumoSimulationSimpleObs\n",
    "from final_simulation._env.qlearn_intersection import SimplestIntersection\n",
    "from final_simulation._sumo.qlearn_simulation import SumoSimulation\n",
    "from final_simulation.sarsa import SARSA_Eval\n",
    "from stable_baselines3 import DQN, PPO, A2C\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Models Eval\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sim time 1 minute\n",
    "n_episodes = 3\n",
    "max_simulation_seconds = 300\n",
    "log_dir = \"final_simulation\\\\_models\\\\eval\"\n",
    "\n",
    "# Simulation\n",
    "simulation = SumoSimulationSimpleObs(\n",
    "    sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo\",\n",
    "    #sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo-gui\",\n",
    "    sumo_config_path=\"C:\\\\sumoconfig\\\\real_intersection.sumocfg\",\n",
    "    signal_states=SignalStates\n",
    ")\n",
    "\n",
    "# Environment\n",
    "env = RealIntersectionSimpleObs13(\n",
    "    simulation=simulation,\n",
    "    max_simulation_seconds=max_simulation_seconds\n",
    ")\n",
    "\n",
    "# Q simulation\n",
    "simulation_q = SumoSimulation(\n",
    "    sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo\",\n",
    "    #sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo-gui\",\n",
    "    sumo_config_path=\"C:\\\\sumoconfig\\\\real_intersection.sumocfg\",\n",
    "    signal_states=SignalStates\n",
    ")\n",
    "\n",
    "# Q Environment\n",
    "env_q = SimplestIntersection(\n",
    "    simulation=simulation_q,\n",
    "    max_simulation_seconds=max_simulation_seconds\n",
    ")\n",
    "\n",
    "# Static Environment\n",
    "env_static = RealIntersectionSimpleObs13_Static(\n",
    "    simulation=simulation,\n",
    "    max_simulation_seconds=max_simulation_seconds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"final_simulation\\\\_models\\\\reward_13\\\\simple\\\\ppo\\\\best_model.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(1,n_episodes + 1):\n",
    "\n",
    "    print(\"Episode: \", episode)\n",
    "\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while done != True:\n",
    "        \n",
    "        action, _states = model.predict(observation)\n",
    "        observation, reward, done, info = env.step(float(action))\n",
    "\n",
    "        if done:\n",
    "            env.save_metrics(\n",
    "                episode=episode,\n",
    "                model_name=\"PPO\",\n",
    "                log_dir=log_dir\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN.load(\"final_simulation\\\\_models\\\\reward_13\\\\simple\\\\dqn\\\\best_model.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(1,n_episodes + 1):\n",
    "\n",
    "    print(\"Episode: \", episode)\n",
    "\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while done != True:\n",
    "        \n",
    "        action, _states = model.predict(observation)\n",
    "        observation, reward, done, info = env.step(float(action))\n",
    "\n",
    "        if done:\n",
    "            env.save_metrics(\n",
    "                episode=episode,\n",
    "                model_name=\"DQN\",\n",
    "                log_dir=log_dir\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C.load(\"final_simulation\\\\_models\\\\reward_13\\\\simple\\\\a2c\\\\best_model.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(1,n_episodes + 1):\n",
    "\n",
    "    print(\"Episode: \", episode)\n",
    "\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while done != True:\n",
    "        \n",
    "        action, _states = model.predict(observation)\n",
    "        observation, reward, done, info = env.step(float(action))\n",
    "\n",
    "        if done:\n",
    "            env.save_metrics(\n",
    "                episode=episode,\n",
    "                model_name=\"A2C\",\n",
    "                log_dir=log_dir\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SARSA_Eval(\n",
    "    q_table_path=\"final_simulation\\\\_models\\\\reward_13\\\\simple\\\\sarsa\\\\final_q_table.npy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  1\n",
      "saving observation file..\n",
      "Episode:  2\n",
      "saving observation file..\n",
      "Episode:  3\n",
      "saving observation file..\n"
     ]
    }
   ],
   "source": [
    "for episode in range(1,n_episodes + 1):\n",
    "\n",
    "    print(\"Episode: \", episode)\n",
    "\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while done != True:\n",
    "        \n",
    "        action = model.predict(observation)\n",
    "        observation, reward, done, info = env.step(float(action))\n",
    "\n",
    "        if done:\n",
    "            env.save_metrics(\n",
    "                episode=episode,\n",
    "                model_name=\"SARSA\",\n",
    "                log_dir=log_dir\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.load(\"final_simulation\\\\_models\\\\reward_13\\\\q-learn\\\\final_q_table.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(1,n_episodes + 1):\n",
    "\n",
    "    print(\"Episode: \", episode)\n",
    "\n",
    "    observation = env_q.reset()\n",
    "    done = False\n",
    "\n",
    "    while done != True:\n",
    "        \n",
    "        #action = model.predict(observation)\n",
    "        action = np.argmax(q_table[observation])\n",
    "        observation, reward, done, info = env_q.step(action)\n",
    "\n",
    "        if done:\n",
    "            env_q.save_metrics(\n",
    "                episode=episode,\n",
    "                model_name=\"QLearn\",\n",
    "                log_dir=log_dir\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(1,n_episodes + 1):\n",
    "\n",
    "    print(\"Episode: \", episode)\n",
    "\n",
    "    observation = env_static.reset()\n",
    "    done = False\n",
    "\n",
    "    while done != True:\n",
    "\n",
    "        observation, reward, done, info = env_static.step(0)\n",
    "\n",
    "        if done:\n",
    "            env_static.save_metrics(\n",
    "                episode=episode,\n",
    "                model_name=\"STATIC\",\n",
    "                log_dir=log_dir\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex Observations & Alternate Reward\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from final_simulation._env.real_intersection import RealIntersection11\n",
    "import final_simulation._sumo.simplest_intersection_simulation as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation\n",
    "simulation_complex = ss.SumoSimulation(\n",
    "    sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo\",\n",
    "    #sumo_binary_path=\"C:\\\\Program Files (x86)\\\\Eclipse\\\\Sumo\\\\bin\\\\sumo-gui\",\n",
    "    sumo_config_path=\"C:\\\\sumoconfig\\\\real_intersection.sumocfg\",\n",
    "    signal_states=SignalStates\n",
    ")\n",
    "\n",
    "# Environment\n",
    "env_complex = RealIntersection11(\n",
    "    simulation=simulation_complex,\n",
    "    max_simulation_seconds=max_simulation_seconds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"final_simulation\\\\_models\\\\reward_11\\\\ppo\\\\best_model.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  1\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "Episode:  2\n",
      "No simulation running.\n",
      "saving observation file..\n",
      "Episode:  3\n",
      "No simulation running.\n",
      "saving observation file..\n"
     ]
    }
   ],
   "source": [
    "for episode in range(1,n_episodes + 1):\n",
    "\n",
    "    print(\"Episode: \", episode)\n",
    "\n",
    "    observation = env_complex.reset()\n",
    "    done = False\n",
    "\n",
    "    while done != True:\n",
    "        \n",
    "        action, _states = model.predict(observation)\n",
    "        observation, reward, done, info = env_complex.step(float(action))\n",
    "\n",
    "        if done:\n",
    "            env_complex.save_metrics(\n",
    "                episode=episode,\n",
    "                model_name=\"PPO_COMPLEX\",\n",
    "                log_dir=log_dir\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C.load(\"final_simulation\\\\_models\\\\reward_11\\\\a2c\\\\best_model.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(1,n_episodes + 1):\n",
    "\n",
    "    print(\"Episode: \", episode)\n",
    "\n",
    "    observation = env_complex.reset()\n",
    "    done = False\n",
    "\n",
    "    while done != True:\n",
    "        \n",
    "        action, _states = model.predict(observation)\n",
    "        observation, reward, done, info = env_complex.step(float(action))\n",
    "\n",
    "        if done:\n",
    "            env_complex.save_metrics(\n",
    "                episode=episode,\n",
    "                model_name=\"A2C_COMPLEX\",\n",
    "                log_dir=log_dir\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DQN.load(\"final_simulation\\\\_models\\\\reward_11\\\\dqn\\\\best_model.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for episode in range(1,n_episodes + 1):\n",
    "\n",
    "#     print(\"Episode: \", episode)\n",
    "\n",
    "#     observation = env_complex.reset()\n",
    "#     done = False\n",
    "\n",
    "#     while done != True:\n",
    "        \n",
    "#         action, _states = model.predict(observation)\n",
    "#         observation, reward, done, info = env_complex.step(float(action))\n",
    "\n",
    "#         if done:\n",
    "#             env_complex.save_metrics(\n",
    "#                 episode=episode,\n",
    "#                 model_name=\"DQN_COMPLEX\",\n",
    "#                 log_dir=log_dir\n",
    "#             )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f344f29490a86472adfcb0d4c68c5dfdcf97bca003fdba9df268b07cb4a6c3e0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
